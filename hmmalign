#!/usr/local/bin/pypy

import optparse
import sys
from collections import defaultdict
import random

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

sys.stderr.write("Training with HMM..." + "\n")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]

# initialize translation probabilities
english_vocab = set()
for (n, (fs, es)) in enumerate(bitext):
  for e in es:
    english_vocab.add(e)
t = defaultdict(lambda: 1.0 / len(english_vocab))

# initialize alignment probabilities
a = {}

count = defaultdict(lambda: 0.0)
total = defaultdict(lambda: 0.0)
s_total = defaultdict(lambda: 0.0)
# adding alignment probability distribution
count_a = defaultdict(lambda: 0.0)
total_a = defaultdict(lambda: 0.0)


l = 0
#while t_old == t:
for l in range(0, 10):
  sys.stderr.write("Iteration " + str(l + 1) + '\n')

  # for every sentence pair
  for (n, (fs, es)) in enumerate(bitext):
    '''
    # compute most likely alignment
    alignment = []
    for j in range(0, len(es)):
      e = es[j]
      prev_a = -1 if j == 0 else alignment[j - 1]
      options = {}
      for i in range(0, len(fs)):
        f = fs[i]
        if (i, prev_a, len(es)) not in a:
          a[(i, prev_a, len(es))] = 1.0 / len(fs)
        options[i] = t[(f, e)] * a[(i, prev_a, len(fs))]
      maxes = [x for x in options.keys() if options[x] == max(options.values())]
      alignment.append(random.choice(maxes))
    '''

    # compute alignment probabilities
    Q = []
    for j in range(0, len(es)):
      Q.append([])
      for i in range(0, len(fs)):
        Q[j].append(t[(f, e)] * max([a[(i, q, len(fs))] * Q[j - 1, q] for q in range(0, len(fs))]))

    # compute normalization
    for j in range(0, len(es)):
      prev_a = -1 if j == 0 else alignment[j - 1]
      e = es[j]
      s_total[e] = 0
      for i in range(0, len(fs)):
        f = fs[i]
        s_total[e] += t[(f, e)] * a[(i, prev_a, len(fs))]

    # collect counts
    for j in range(0, len(es)):
      prev_a = -1 if j == 0 else alignment[j - 1]
      e = es[j]
      for i in range(0, len(fs)):
        f = fs[i]
        c = (t[(f, e)] * a[(i, prev_a, len(fs))]) / s_total[e]
        count[(f,e)] += c
        total[f] += c
        count_a[i - prev_a] += c

  # estimate probabilities
  for (k, (f, e)) in enumerate(count.keys()):
    t[(f, e)] = count[(f, e)] / total[f]

  for (k, (i, prev_a, l_e)) in enumerate(a.keys()):
    a[(i, prev_a, l_e)] = count_a[i - prev_a] / sum([count_a[q - prev_a] for q in range(0, l_e)])

# print out alignments
for (fs, es) in bitext:
  alignment = []
  for (j, e) in enumerate(es):
    prev_a = -1 if j == 0 else alignment[j - 1]
    options = {}
    for (i, f) in enumerate(fs): 
      if (i, prev_a, len(fs)) not in a:
        a[(i, prev_a, len(fs))] = 1.0 / len(es)
      options[i] = t[(f, e)] * a[(i, prev_a, len(fs))]
    maxes = [x for x in options.keys() if options[x] == max(options.values())]
    argmax = random.choice(maxes)
    alignment.append(argmax)
    sys.stdout.write("%i-%i " % (argmax,j))
  sys.stdout.write('\n')
